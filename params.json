{"name":"Cassandradump","tagline":"A data exporting tool for Cassandra inspired from mysqldump, with some additional slice and dice capabilities","body":"# cassandradump\r\n\r\n## Description\r\n\r\nA data exporting tool for Cassandra inspired from mysqldump, with some additional slice and dice capabilities.\r\n\r\nDisclaimer: most of the times, you really shouldn't be using this. It's fragile, non-scalable, inefficient and verbose. Cassandra already offers excellent exporting/importing tools:\r\n\r\n- Snapshots\r\n- CQL's COPY FROM/TO\r\n- sstable2json\r\n\r\nHowever, especially during development, I frequently need to:\r\n\r\n- Quickly take a snapshot of an entire keyspace, and import it just as quickly without copying too many files around or losing too much time\r\n- Ability to take a very small subset of a massive production database (according to some CQL-like filtering) and import it quickly on my development environment\r\n\r\nIf these use cases sound familiar, this tool might be useful for you.\r\n\r\nIt's still missing many major Cassandra features that I don't use daily, so feel free to open an issue pointing them out (or send a pull request) if you need something.\r\n\r\n## Usage\r\n\r\nThe help should already contain some useful information:\r\n\r\n```\r\n$ python cassandradump.py --help\r\nusage: cassandradump.py [-h] [--host HOST] [--keyspace KEYSPACE] [--cf CF]\r\n                        [--filter FILTER] [--no-insert] [--no-create]\r\n                        [--import-file IMPORT_FILE]\r\n                        [--export-file EXPORT_FILE] [--quiet]\r\n\r\nA data exporting tool for Cassandra inspired from mysqldump, with some added\r\nslice and dice capabilities.\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  --host HOST           the address of a Cassandra node in the cluster\r\n                        (localhost if omitted)\r\n  --keyspace KEYSPACE   export a keyspace along with all its column families.\r\n                        Can be specified multiple times\r\n  --cf CF               export a column family. The name must include the\r\n                        keyspace, e.g. \"system.schema_columns\". Can be\r\n                        specified multiple times\r\n  --filter FILTER       export a slice of a column family according to a CQL\r\n                        filter. This takes essentially a typical SELECT query\r\n                        stripped of the initial \"SELECT ... FROM\" part (e.g.\r\n                        \"system.schema_columns where keyspace_name\r\n                        ='OpsCenter'\", and exports only that data. Can be\r\n                        specified multiple times\r\n  --no-insert           don't generate insert statements\r\n  --no-create           don't generate create (and drop) statements\r\n  --import-file IMPORT_FILE\r\n                        import data from the specified file\r\n  --export-file EXPORT_FILE\r\n                        export data to the specified file\r\n  --quiet               quiet progress logging\r\n```\r\n\r\nIn its simplest invocation, it exports data and schemas for all keyspaces:\r\n\r\n```\r\n$ python cassandradump.py --export-file dump.cql\r\nExporting all keyspaces\r\nExporting schema for keyspace OpsCenter\r\nExporting schema for column family OpsCenter.events_timeline\r\nExporting data for column family OpsCenter.events_timeline\r\nExporting schema for column family OpsCenter.settings\r\nExporting data for column family OpsCenter.settings\r\nExporting schema for column family OpsCenter.rollups60\r\nExporting data for column family OpsCenter.rollups60\r\n...\r\n```\r\n\r\n```\r\n$ cat dump.cql\r\nDROP KEYSPACE IF EXISTS \"OpsCenter\";\r\nCREATE KEYSPACE \"OpsCenter\" WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;\r\nDROP TABLE IF EXISTS \"OpsCenter\".\"events_timeline\";\r\nCREATE TABLE \"OpsCenter\".events_timeline (key text, column1 bigint, value blob, PRIMARY KEY (key, column1)) WITH COMPACT STORAGE AND CLUSTERING ORDER BY (column1 ASC) AND caching = '{\"keys\":\"ALL\", \"rows_per_partition\":\"NONE\"}' AND comment = '{\"info\": \"OpsCenter management data.\", \"version\": [5, 1, 0]}' AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '8'} AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'} AND dclocal_read_repair_chance = 0.0 AND default_time_to_live = 0 AND gc_grace_seconds = 864000 AND max_index_interval = 2048 AND memtable_flush_period_in_ms = 0 AND min_index_interval = 128 AND read_repair_chance = 0.25 AND speculative_retry = 'NONE';\r\nINSERT INTO \"OpsCenter\".\"events_timeline\" (key, column1, value) VALUES ('201501', 1419841027332869, 0x)\r\nINSERT INTO \"OpsCenter\".\"events_timeline\" (key, column1, value) VALUES ('201501', 1419841027352525, 0x)\r\nINSERT INTO \"OpsCenter\".\"events_timeline\" (key, column1, value) VALUES ('201501', 1419928979070954, 0x)\r\n...\r\n```\r\n\r\nThe created dump file can be directly used with ```cqlsh -f```, or there's also a ```--import-file``` switch because I noticed that sometimes cqlsh gets stuck if a big file is passed as input (probably a temporary bug).\r\n\r\nUsing ```--keyspace```, it's possible to filter for a specific set of keyspaces\r\n\r\n```\r\ngianluca@sid:~$ python cassandradump.py --keyspace system --export-file dump.cql\r\nExporting schema for keyspace system\r\nExporting schema for column family system.peers\r\nExporting data for column family system.peers\r\nExporting schema for column family system.range_xfers\r\nExporting data for column family system.range_xfers\r\nExporting schema for column family system.schema_columns\r\nExporting data for column family system.schema_columns\r\n...\r\n```\r\n\r\n```\r\n$ cat dump.cql\r\nDROP KEYSPACE IF EXISTS \"system\";\r\nCREATE KEYSPACE system WITH replication = {'class': 'LocalStrategy'}  AND durable_writes = true;\r\nDROP TABLE IF EXISTS \"system\".\"peers\";\r\nCREATE TABLE system.peers (peer inet PRIMARY KEY, data_center text, host_id uuid, preferred_ip inet, rack text, release_version text, rpc_address inet, schema_version uuid, tokens set<text>) WITH bloom_filter_fp_chance = 0.01 AND caching = '{\"keys\":\"ALL\", \"rows_per_partition\":\"NONE\"}' AND comment = 'known peers in the cluster' AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32'} AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'} AND dclocal_read_repair_chance = 0.0 AND default_time_to_live = 0 AND gc_grace_seconds = 0 AND max_index_interval = 2048 AND memtable_flush_period_in_ms = 3600000 AND min_index_interval = 128 AND read_repair_chance = 0.0 AND speculative_retry = '99.0PERCENTILE';\r\n...\r\n```\r\n\r\nUsing ```--cf```, it's possible to filter for a specific set of column families:\r\n\r\n```\r\ngianluca@sid:~$ python cassandradump.py --cf OpsCenter.rollups7200 --no-create --export-file dump.cql\r\nExporting schema for column family OpsCenter.rollups7200\r\nExporting data for column family OpsCenter.rollups7200\r\n```\r\n\r\n```\r\n$ cat dump.cql\r\nINSERT INTO \"OpsCenter\".\"rollups7200\" (key, column1, value) VALUES ('127.0.0.1-foo', 718946047, 0x000000000000000000000000)\r\nINSERT INTO \"OpsCenter\".\"rollups7200\" (key, column1, value) VALUES ('127.0.0.1-foo', 718953247, 0x000000000000000000000000)\r\nINSERT INTO \"OpsCenter\".\"rollups7200\" (key, column1, value) VALUES ('127.0.0.1-foo', 718960447, 0x000000000000000000000000)\r\nINSERT INTO \"OpsCenter\".\"rollups7200\" (key, column1, value) VALUES ('127.0.0.1-foo', 718967647, 0x000000000000000000000000)\r\nINSERT INTO \"OpsCenter\".\"rollups7200\" (key, column1, value) VALUES ('127.0.0.1-foo', 719032447, 0x40073fc200000000437bc000)\r\n...\r\n```\r\n\r\nUsing ```--no-insert``` and ```--no-create``` it's possible to tweak what CQL statements are actually included in the dump.\r\n\r\nMost of the times, the column families in a production scenario are huge, and you might just want a little slice of it. With ```--filter```, it's possible to specify a set of CQL filters, and just the data that satisfies those filters will be included in the dump:\r\n\r\n```\r\ngianluca@sid:~$ python cassandradump.py  --filter \"system.schema_columns WHERE keyspace_name='OpsCenter'\" --export-file dump.cql\r\nExporting data for filter \"system.schema_columns where keyspace_name ='OpsCenter'\"\r\n```\r\n\r\n```\r\n$ cat dump.cql\r\nINSERT INTO \"system\".\"schema_columns\" (keyspace_name, columnfamily_name, column_name, component_index, index_name, index_options, index_type, type, validator) VALUES ('OpsCenter', 'backup_reports', 'backup_id', 1, NULL, 'null', NULL, 'clustering_key', 'org.apache.cassandra.db.marshal.UTF8Type')\r\nINSERT INTO \"system\".\"schema_columns\" (keyspace_name, columnfamily_name, column_name, component_index, index_name, index_options, index_type, type, validator) VALUES ('OpsCenter', 'backup_reports', 'deleted_at', 4, NULL, 'null', NULL, 'regular', 'org.apache.cassandra.db.marshal.TimestampType')\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}